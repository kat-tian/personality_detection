{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "personality_XGboost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kat-tian/personality_detection/blob/master/personality_XGboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoje3OcUegNP",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing and XGboost for data \n",
        "\n",
        "1. Import dataset (essays.csv)\n",
        "2. Load embeddings (Mount to Drive)\n",
        "3. Function to clean data (strip, lower, non-ASCII)\n",
        "4. Lemmatize \n",
        "5. Vectorize (vector of each word, sum arrays for each word: one number/word)\n",
        "6. Set vectors to fixed-length (pad short, cut long)\n",
        "7. Convert categorical lables (y/n to 0/1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI81fWMpf2sE",
        "colab_type": "toc"
      },
      "source": [
        ">[Preprocessing and XGboost for data](#scrollTo=Yoje3OcUegNP)\n",
        "\n",
        ">>[Setup Installs](#scrollTo=BqCCKE4IgY47)\n",
        "\n",
        ">>[Data Preprocessing](#scrollTo=fG-OpEPHwzD0)\n",
        "\n",
        ">>>[Define Functions](#scrollTo=fG-OpEPHwzD0)\n",
        "\n",
        ">>>[Prepare Train/Test](#scrollTo=LRsxLQeJ_BXd)\n",
        "\n",
        ">>[Build XGBoost Model](#scrollTo=7boYySSukXJo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqCCKE4IgY47",
        "colab_type": "text"
      },
      "source": [
        "## Setup Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-fImeX_QsId",
        "colab_type": "code",
        "outputId": "fa716b6d-c38e-4700-9daa-c6e54ce96066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#standard imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re \n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lzJZIwHSQYi",
        "colab_type": "code",
        "outputId": "4f279f97-fab0-4c5b-e78a-c36711c7bbc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33WNLe-lOGr",
        "colab_type": "code",
        "outputId": "a3d675ba-1aa6-463a-de2d-3992bb749326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# import embeddings\n",
        "from gensim.models import KeyedVectors\n",
        "filename = '/content/drive/My Drive/trained_models/GoogleNews-vectors-negative300.bin.gz'\n",
        "emb_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG-OpEPHwzD0",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Define Functions\n",
        "1. text_preprocess: tokenzie, to lower, remove punct,non-ASCII, and numbers (save as 'CLEAN_TEXT')\n",
        "2. lemmatize: using list returned from previous, lemmatize (update 'CLEAN_TEXT')\n",
        "3. get_vectors: use clean_text list, apply get vectors to get array for each word, then sum array each word (save as 'TEXT_VECTORS')\n",
        "4. count_list: count length of lists for each value \n",
        "5. fixed_vector: pad short vect, cut long (save as 'FIXED_VECTORS')\n",
        "\n",
        "\n",
        "Finally, encode the categorical data to 0/1 using a mapping. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWKiUIWRPK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reading our data\n",
        "data = pd.read_csv('essays.csv', encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti89_OoRS0-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define all functions \n",
        "\n",
        "def text_preprocess(text_str):\n",
        "  \"\"\" Takes string as input, tokenizes, remove punct, \n",
        "      to lower, strip, and remove stop words\"\"\"\n",
        "  \n",
        "  text = text_str.strip().lower()\n",
        "  text = re.sub('<[^<]+?>','', text)\n",
        "  text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
        "  text = ''.join(c for c in text if not c.isdigit())\n",
        "  text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "  tokens = word_tokenize(text)\n",
        "  result = [i for i in tokens if i not in stop_words]\n",
        "  return result\n",
        "\n",
        "\n",
        "def lemmatize(list):\n",
        "  \"\"\"Take the list returned from previous function\n",
        "  lemmatize\"\"\"\n",
        "  \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemma = [lemmatizer.lemmatize(x) for x in list]\n",
        "  return lemma\n",
        "\n",
        "\n",
        "def get_vectors(list):\n",
        "  \"\"\"For word in list, get embedding,\n",
        "  add embeddings for word, return all words in list\"\"\"\n",
        "\n",
        "  vectors = []\n",
        "  for word in list: \n",
        "    try:\n",
        "      vector = emb_model[word]\n",
        "      vector = np.sum(vector)\n",
        "      vectors.append(vector)\n",
        "      \n",
        "    except KeyError: \n",
        "      pass\n",
        "     \n",
        "  return vectors\n",
        "\n",
        "\n",
        "def count_list(list):\n",
        "  \"\"\"count items in list, return count\"\"\"\n",
        "  return len(list)\n",
        "\n",
        "\n",
        "#PROBLEM WITH THIS FUNCTION \n",
        "def fixed_vector(vec_list):\n",
        "  \"\"\"create fixed length vectors for model input\"\"\"\n",
        "  \n",
        "  if len(vec_list)>600:\n",
        "    vec_list = vec_list[:600]\n",
        "    \n",
        "  elif len(vec_list)<600: \n",
        "    difference = 600-int(len(vec_list))\n",
        "    vec_list.extend([0.0] * difference)\n",
        "    \n",
        "  return vec_list\n",
        "\n",
        "def sum_vectors(vec_list):\n",
        "  \"\"\"Take dense vector list as input\n",
        "  return the sum elementwise\"\"\"\n",
        "  return np.sum(vec_list, axis=0)\n",
        "\n",
        "def dense_vectors(list):\n",
        "  \"\"\"For word in list, get embedding,\n",
        "  add embeddings for word, return all words in list\"\"\"\n",
        "\n",
        "  vectors = []\n",
        "  for word in list: \n",
        "    try:\n",
        "      vector = emb_model[word]\n",
        "      vectors.append(vector)\n",
        "      \n",
        "    except KeyError: \n",
        "      pass\n",
        "     \n",
        "  return vectors\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCLWu6crXxXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply functions \n",
        "data['CLEAN_TEXT'] = data['TEXT'].apply(text_preprocess) #clean text\n",
        "data['CLEAN_TEXT'] = data['CLEAN_TEXT'].apply(lemmatize) #lemmatize\n",
        "data['DENSE_VECTORS'] = data['CLEAN_TEXT'].apply(dense_vectors)\n",
        "data['TEXT_VECTORS'] = data['CLEAN_TEXT'].apply(get_vectors) #create vectors\n",
        "data['VECTOR_COUNT'] = data['TEXT_VECTORS'].apply(count_list) #count len vectors\n",
        "data['FIXED_VECTORS'] = data['TEXT_VECTORS'].apply(fixed_vector) #vecs to fix length \n",
        "data['FIXED_VEC_COUNT'] = data['FIXED_VECTORS'].apply(count_list) #len of fix-vec list\n",
        "data['SUM_ELEMENTWISE_VECTORS'] = data['DENSE_VECTORS'].apply(sum_vectors) #sum-vectors are the elementwise sum for dense vectors \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XUuT5k9w9Te",
        "colab_type": "code",
        "outputId": "a9f6d40d-fb47-43db-f5fc-f70b1ed26a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#encode categorical data to 0,1\n",
        "cleanup_nums = {'y': 1.0, 'n': 0.0}\n",
        "data.replace(cleanup_nums, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py:1922: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  op = lambda x: operator.eq(x, b)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqhDIwmHo2bf",
        "colab_type": "code",
        "outputId": "7e7e5b14-cb18-481f-a4ac-0500d19dc4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data['FIXED_VEC_COUNT'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600    2467\n",
              "Name: FIXED_VEC_COUNT, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5gjpqDifFFW",
        "colab_type": "code",
        "outputId": "0eebe1a5-a52a-46ab-9235-dd235dab2ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(data.sample(2))\n",
        "print('len:', len(data['TEXT']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              #AUTHID  ...                            SUM_ELEMENTWISE_VECTORS\n",
            "2416     2004_390.txt  ...  [11.978897, 14.949148, 6.8908997, 76.28322, -3...\n",
            "1804  2002_819526.txt  ...  [17.69645, 13.626469, 7.5613594, 38.349854, -2...\n",
            "\n",
            "[2 rows x 14 columns]\n",
            "len: 2467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmry9UgBh4_N",
        "colab_type": "code",
        "outputId": "96d139e7-bd26-4260-bb02-e51eb4c06464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "data['FIXED_VECTORS'][0:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [-0.59524536, -0.46329117, 7.6182823, -3.18063...\n",
              "1    [-0.59524536, 2.0501537, 0.39393616, 0.7791672...\n",
              "Name: FIXED_VECTORS, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRsxLQeJ_BXd",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Train/Test\n",
        "Prepare input and labels. Fix the dimensions, pad missing values, and check the stuctures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N53TO0G8jp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#features and labels\n",
        "features = np.matrix([0]*300)\n",
        "for val in data['SUM_ELEMENTWISE_VECTORS'].values:\n",
        "  features = np.append(features, [val], axis=0)\n",
        "features = features[1:]\n",
        "  \n",
        "labels_ext = data['cEXT'].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa1VTfplnr4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train test/split using sklearn \n",
        "seed = 22\n",
        "test_size = 0.30\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels_ext, test_size=test_size, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwssmdm3-0ye",
        "colab_type": "code",
        "outputId": "dc14ecef-40e3-43a3-b246-b5629a10d51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#check shapes\n",
        "print('y_test shape:', y_test.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_test shape: (741,)\n",
            "x_test shape: (741, 300)\n",
            "x_train shape: (1726, 300)\n",
            "y_train shape: (1726,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7boYySSukXJo",
        "colab_type": "text"
      },
      "source": [
        "## Build XGBoost Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmCnVPKpkWVp",
        "colab_type": "code",
        "outputId": "1ed4dd97-9a1b-4296-9753-63cc48d62f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# fit model no training data\n",
        "model = XGBClassifier()\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y5hXrQnlQB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A2LpA6rlbrK",
        "colab_type": "code",
        "outputId": "fb98de94-7c21-4538-d7b4-cfa1467c8a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 52.77%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oluXtZJ-Bt1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1BLcqzWZhxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}